# -*- coding: utf-8 -*-
"""Melb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ct_4BXHP68auJoS_lVcYMp3TC2UYuBxU

# **Melbourne House Price prediction**

## **Importing** **libraries** **bold text**
"""

import os
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_regression
from sklearn.impute import SimpleImputer

from xgboost import XGBRegressor
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor
from lightgbm import LGBMRegressor

from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import GridSearchCV

from .score import scored
from .correct_data import correct
from .impute_data import impute
from .loading_data import load_data
from .feature_model import make_mi, model_select, Lazy
from .hyperparams import tunning, objective_HGBR, objective_LGBMR



plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
       "axes",
       labelweight = "bold",
       labelsize = "large",
       titleweight = "bold",
       titlesize = 14,
       titlepad = 10,
       )

warnings.filterwarnings('ignore')

"""##Installing and importing **lazypredict** (A tool for model performance predictions)"""

!pip install lazypredict

from lazypredict.Supervised import LazyRegressor

"""## Uploading the Melbourne dataset"""


import io

def main():


    from google.colab import files
    uploaded = files.upload()

    dataset = pd.read_csv(io.BytesIO(uploaded['Melbourne_housing_FULL.csv']))
    # or for local data
    # dataset = pd.read_csv("/home/alassane/Documents/DATA/Melbourne/Melbourne_housing_FULL.csv")

    """List of categorical cols"""

    dataset.select_dtypes(include = ["object"]).columns

    councilarea = dataset["CouncilArea"].unique()
    councilarea

    """## We'll firstly correct our dataset by dropping unecessary columns and then impute it by filing NaN values after this we'll normalize the dataset

    Specifying the path to the dataset directory**

    Precising the new "Date" datetime column name and the "years" column name**

    Adding the columns which are supposed to be dropped**
    """

    path = "/home/alassane/Documents/DATA/Melbourne/Melbourne_housing_FULL.csv"
    new_date_col, year_col = "new date", "date_year"
    cols_to_drop = ["Address", "SellerG", "Regionname", "Lattitude", "Longtitude", "Date", "new date", "Suburb"]

    """# **Print the dataset after correcting**"""

    correct(path, new_date_col, year_col, cols_to_drop)

    """***Now , we will fix NaN values and do some Encoding to few categorical variables***

    Cols to encode : "Type", "CouncilArea", "Method
    """

    OHE_cols = ["Type", "CouncilArea", "Method"]

    impute(dataset, OHE_cols)

    """# The loading the dataset

    *Indicate the target feature and set values for random state and the test size*
    """

    target_col = "Price"
    random_state = 0
    test_size = 0.3


    load_data(target_col, random_state, test_size)

    X_train, X_val, y_train, y_val = load_data(target_col, random_state, test_size)

    """##We'll use an amazing tool from sklearn (mutual information) to see how important each feature in the training set is"""

    mi_scores = make_mi(X_train, y_train)

    """**Printing the MI-Score percentage**"""

    print(mi_scores*100)

    """**We can throw away non informative features**"""

    informative = ["Postcode", "Propertycount", "Distance", "Rooms", "Bedroom2", "Type_u", "Method_S"]
    X_view = X_train[informative]
    data_view = pd.concat([X_view, y_train.reindex(X_view.index)], axis = 1)

    # We'll use data_view for visualization purpose only
    data_view = data_view.iloc[:25, :]

    """##Now we'll use lazypredict to have some ideas about best models that fit well
    ##our dataset
    """

    reg = LazyRegressor(ignore_warnings=False, custom_metric=None)
    models, predictions = reg.fit(X_train.iloc[:10000, :], X_val.iloc[:10000, :],y_train[:10000], y_val[:10000])
    print(models)

    # Data Visualization

    # Barplot
    plt.figure(figsize = (14, 7))
    sns.barplot(x = X_view.iloc[:30, :].index, y = y_train[:30])

    # Heatmap
    plt.figure(figsize = (15, 20))
    sns.heatmap(data = X_view.iloc[:45, :], annot = True)

    # lineplot
    plt.figure(figsize = (14, 7))
    sns.lineplot(data = X_view.iloc[:20, :])

    # scatterplot
    plt.figure(figsize = (14, 7))
    sns.scatterplot(x = X_view["Postcode"], y = y_train)

    #regplot
    plt.figure(figsize = (14, 7))
    sns.regplot(x = X_view["Postcode"], y = y_train)

    plt.figure(figsize = (14, 7))
    sns.scatterplot(x = X_view["Postcode"], y = y_train, hue = X_view["Propertycount"])

    # lmplot
    plt.figure(figsize = (20, 30))
    sns.lmplot(x = "Postcode", y = "Price", hue ="Rooms", data = data_view)

    # swarmplot
    plt.figure(figsize = (14, 7))
    sns.swarmplot(x = X_view["Postcode"], y = y_train)

    #displot
    plt.figure(figsize = (14, 7))
    sns.distplot(a = X_view["Postcode"], label = "Postcode", kde=False)

    #kdeplot
    plt.figure(figsize = (14, 7))
    sns.kdeplot(x = X_view["Postcode"], shade = True)

    # jointplot
    plt.figure(figsize = (4, 4))
    sns.jointplot(x = X_view["Postcode"], y = y_train, kind = "kde")


    """The model_selection function:

    **Adding the name, the score and the Mean Absolute Error of any model used**
    """

    models = [GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor,
             XGBRegressor, LGBMRegressor]

    selection = model_select(models, X_train, y_train, X_val, y_val)
    print(selection)

    """**Now we can see the performance of each model. 
    HistGradientBoostingRegressor is doing well.  
    Let's make some predictions**

    """

    mode = HistGradientBoostingRegressor()
    M = mode.fit(X_train, y_train)

    mse1 = mean_absolute_error(y_train, M.predict(X_train))
    print("Training: ", mse1)

    mse2 = mean_absolute_error(y_val, M.predict(X_val))
    print("Testing: ", mse2)

    """***Compare to other models , we've achieved some acceptable score
    Now it is time to tune our models in order to improve the performance***

    ---

    Now we are defining the tunning function as we must try differents models
    """

    """**Let's use optuna as GridSearch takes a lot of time for processing**"""

    """**Optuna use case with HistGradientBoostingRegressor model**"""

    study = optuna.create_study(direction = "minimize")
    study.optimize(objective_HGBR, n_trials = 50)
    HGBR_params = study.best_params

    """**We've got the best parameters of our model that we can use for future tests**"""

    HGBRmode = LGBMRegressor(**HGBR_params)

    """**Optuna use case for LGBMRegressor model**"""

    study = optuna.create_study(direction = "minimize")
    study.optimize(objective_LGBMR, n_trials = 20)
    LGBMR_params = study.best_params

    LGBMfinal = LGBMRegressor(**LGBMR_params)

    LGBMfinal.fit(X_train, np.log(y_train))

if __name__ == '__main__':

    mean_absolute_error(y_val, np.exp(LGBMfinal.predict(X_val)))

    """Tunning Done â–¶
    It's time to verify if we've done some remarkable work

    **We've slightly made impeovements but we can keep trying to change the params provided to optuna**
    """

    mse1 = mean_absolute_error(y_train, np.exp(LGBMfinal.predict(X_train)))
    print("Training: ", mse1)

    mse2 = mean_absolute_error(y_val, np.exp(LGBMfinal.predict(X_val)))
    print("Testing: ", mse2)